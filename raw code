from collections import defaultdict
import numpy as np
import math
class LSTM():
    def __init__(self, num_layers=1, timesteps=10, hidden_size=5):
        self.loss = 'binary_crossentropy'
        self.initialmethod = 'he'
        self.layerstype = []
        self.activationsdense = []
        self.inputshape = None
        self.weightsdense = defaultdict(lambda: np.array([]))
        self.biasdense = defaultdict(lambda: np.array([]))
        self.forgetweight = defaultdict(lambda: np.array([]))
        self.inputweight = defaultdict(lambda: np.array([]))
        self.cellweight = defaultdict(lambda: np.array([]))
        self.outputweight = defaultdict(lambda: np.array([]))
        self.forgetbias = defaultdict(lambda: np.array([]))
        self.inputbias = defaultdict(lambda: np.array([]))
        self.cellbias = defaultdict(lambda: np.array([]))
        self.outputbias = defaultdict(lambda: np.array([]))
        self.hidden_forgetweight = defaultdict(lambda: np.array([]))
        self.hidden_inputweight = defaultdict(lambda: np.array([]))
        self.hidden_cellweight = defaultdict(lambda: np.array([]))
        self.hidden_outputweight = defaultdict(lambda: np.array([]))
        self.activationvalues = defaultdict(lambda: np.array([]))
        self.layercountdense = 0
        self.layercountlstm = 0
        self.neurons = []

        self.num_layers = num_layers
        self.timesteps = timesteps
        self.hidden_size = hidden_size
        self.ht = np.zeros((timesteps, num_layers, hidden_size))
        self.ct = np.zeros((timesteps, num_layers, hidden_size))
        self.ft = np.zeros((timesteps, num_layers, hidden_size))
        self.it = np.zeros((timesteps, num_layers, hidden_size))
        self.ot = np.zeros((timesteps, num_layers, hidden_size))
        self.ct_candidate = np.zeros((timesteps, num_layers, hidden_size))

        self.optimizers = {
            'forget': defaultdict(lambda: Optimizer()),
            'input': defaultdict(lambda: Optimizer()),
            'cell': defaultdict(lambda: Optimizer()),
            'output': defaultdict(lambda: Optimizer()),
            'dense': defaultdict(lambda: Optimizer())
            }

    def weightinitializationDense(self, shape):
        if self.initialmethod == 'he':
            limit = np.sqrt(2 / shape[0])
        elif self.initialmethod == 'xavier':
            limit = np.sqrt(6 / sum(shape))
        else:
            raise ValueError("Unsupported initialization method. Choose 'xavier' or 'he'")
        return np.random.uniform(-limit, limit, size=shape)

    def weightinitializationLSTM(self, shape):
        if self.initialmethod == 'he':
            limit = np.sqrt(2 / shape[0])
        elif self.initialmethod == 'xavier':
            limit = np.sqrt(6 / sum(shape))
        else:
            raise ValueError("Unsupported initialization method. Choose 'xavier' or 'he'")
        return np.random.uniform(-limit, limit, size=shape)

    def biasinitialization(self, n):
        return np.zeros((1, n))

    def add(self, typ, n, activation='tanh', input_shape=None):
        if typ == 'Dense':
            self.layerstype.append(typ)
            if self.layercountdense == 0 and input_shape is not None:
                self.neurons.append(input_shape[1])
            self.neurons.append(n)
            self.activationsdense.append(activation)
            self.weightsdense[self.layercountdense] = self.weightinitializationDense((self.neurons[-2], self.neurons[-1]))
            self.biasdense[self.layercountdense] = np.zeros((1, self.neurons[-1]))
            self.layercountdense += 1

        elif typ == 'LSTM':
            self.layerstype.append(typ)
            if self.layercountlstm == 0 and input_shape is not None:
                self.neurons.append(input_shape[1])
            self.neurons.append(n)

            self.forgetweight[self.layercountlstm] = self.weightinitializationLSTM((self.neurons[-2], self.neurons[-1]))
            self.inputweight[self.layercountlstm] = self.weightinitializationLSTM((self.neurons[-2], self.neurons[-1]))
            self.cellweight[self.layercountlstm] = self.weightinitializationLSTM((self.neurons[-2], self.neurons[-1]))
            self.outputweight[self.layercountlstm] = self.weightinitializationLSTM((self.neurons[-2], self.neurons[-1]))

            self.forgetbias[self.layercountlstm] = self.biasinitialization(self.neurons[-1])
            self.inputbias[self.layercountlstm] = self.biasinitialization(self.neurons[-1])
            self.cellbias[self.layercountlstm] = self.biasinitialization(self.neurons[-1])
            self.outputbias[self.layercountlstm] = self.biasinitialization(self.neurons[-1])

            self.hidden_forgetweight[self.layercountlstm] = self.weightinitializationLSTM((self.neurons[-1], self.neurons[-1]))
            self.hidden_inputweight[self.layercountlstm] = self.weightinitializationLSTM((self.neurons[-1], self.neurons[-1]))
            self.hidden_cellweight[self.layercountlstm] = self.weightinitializationLSTM((self.neurons[-1], self.neurons[-1]))
            self.hidden_outputweight[self.layercountlstm] = self.weightinitializationLSTM((self.neurons[-1], self.neurons[-1]))

            self.layercountlstm += 1

    def activations(self, x, activation):
        if activation == 'linear':
            return x
        elif activation == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif activation == 'tanh':
            return np.tanh(x)
        elif activation == 'relu':
            return np.maximum(0, x)
        elif activation == 'leakyrelu':
            return np.maximum(0.01 * x, x)
        elif activation == 'softmax':
            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
            return exp_x / np.sum(exp_x, axis=1, keepdims=True)
        else:
            raise ValueError("Unsupported activation function.")
    def forwardpropagationLSTM(self,x,timestep,z):
       ft=np.dot(self.forgetweight[z],x)+np.dot(self.hidden_forgetweight[z],self.ht[timestep-1,z])+self.forgetbias[z]
       ft=self.activations(ft,'sigmoid')


       it=np.dot(self.inputweight[z],x)+np.dot(self.hidden_inputweight[z],self.ht[timestep-1,z])+self.inputbias[z]
       it=self.activations(it,'sigmoid')


       ctcandidate=np.dot(self.cellweight[z],x)+np.dot(self.hidden_cellweight[z],self.ht[timestep-1,z])+self.cellbias[z]
       ctcandidate=self.activations(ctcandidate,'tanh')


       ot=np.dot(self.outputweight[z],x)+np.dot(self.hidden_outputweight[z],self.ht[timestep-1,z])+self.outputbias[z]
       ot=self.activations(ot,'sigmoid')


       ct=ft*self.ct[timestep-1,z]+it*ctcandidate
       ht = ot * np.tanh(ct)

       self.ct[timestep,z]=ct
       self.ht[timestep,z]=ht

       self.ft[timestep,z]=ft
       self.it[timestep,z]=it
       self.ct_candidate[timestep,z]=ctcandidate
       self.ot[timestep,z]=ot



    def forwardpropagationDense(self, h, layer):
       z = np.dot(self.weights[layer], h )+ self.biases[layer]
       h_new = self.activations(z, self.activationsdense[layer])
       self.activationvalues[layer]=h_new
       return h_new
    def lossderivative(self,ytrue,ypred):
      if self.loss == 'binary_crossentropy' or self.loss == 'categorical_crossentropy':
        return ypred-ytrue
      elif self.loss == 'mean_squared_error':
        return 2*(ypred-ytrue)
      elif self.loss == 'mean_absolute_error':
        return np.sign(ypred-ytrue)
      else:
        raise ValueError("Unsupported loss function. Choose 'binary_crossentropy', 'categorical_crossentropy', or 'mean_squared_error'.")
    def activationderivative(self, x, activation):
        if activation == 'linear':
            return np.ones_like(x)
        elif activation == 'sigmoid':
            sig = self.activation(x, 'sigmoid')
            return sig * (1 - sig)
        elif activation == 'tanh':
            return 1 - np.tanh(x) ** 2
        elif activation == 'relu':
            return np.where(x > 0, 1, 0)
        elif activation == 'leakyrelu':
            return np.where(x > 0, 1, 0.01)
        else:
            raise ValueError("Unsupported activation function.")
    def backpropagationdense(self,delta,layer,learning_rate):
      delta *= self.activationderivative(self.activationvalues[layer + 1], self.activations[j])
      batch_size = self.activationvalues[layer].shape[0]
      dw = np.dot(self.activationvalues[layer].T, delta) / batch_size
      db = np.sum(delta, axis=0, keepdims=True) / batch_size

      return dw, db, np.dot(delta, self.weightsdense[layer].T)
    def backpropagationLSTM(self,delta,dcell,timestep,layer,learning_rate,x,optimizer_type='adam'):
         df=dcell*self.ct[timestep-1,layer]*self.activationderivative(self.ft[timestep,layer],'sigmoid')
         di=dcell*self.ct_candidate[timestep,layer]*self.activationderivative(self.it[timestep,layer],'sigmoid')
         dc_candidate=dcell*self.it[timestep,layer]*self.activationderivative(self.ct_candidate[timestep,layer],'tanh')
         do=delta*np.tanh(self.ct[timestep,layer])*self.activationderivative(self.ot[timestep,layer],'sigmoid')


        dwf=np.dot(df.T,x[:,timestep,:])
        dwi=np.dot(di.T,x[:,timestep,:])
        dwc_candidate=np.dot(dc_candidate.T,x[:,timestep,:])
        dwo=np.dot(do.T,x[:,timestep,:])

        return dwf,dwi,dwc_candidate,dwo



    def fit(self,x,y,epochs,learning_rate):
      self.x=x
      for epoch in range(epochs):
        batch_size=x.shape[0]
        for layer in range(self.layercountlstm):
                self.ht[0,layer] = np.zeros((batch_size, self.neurons[layer+1]))
                self.ct[0,layer] = np.zeros((batch_size, self.neurons[layer+1]))

        for timestep in range(1,x.shape[1]):
          for layer in range(self.layercountlstm):
            if layer==0:
              self.forwardpropagationLSTM(x[:,timestep,:],timestep,layer)
            else:
              self.forwardpropagationLSTM(self.ht[timestep,layer-1],timestep,layer)

        denseinput=self.ht[x.shape[1]-1,self.layercountlstm-1]
        for layer in range(self.layercountdense):
            denseinput=self.forwardpropagationdense(denseinput,layer)
        delta=self.lossderivative(y,denseinput)
        for layer in range(self.layercountdense):
          dw, db, delta = self.backpropagationdense(delta, layer, learning_rate, optimizer_type)
          self.weightsdense[layer], self.biasdense[layer] = self.optimizers['dense'][layer].optimizer(
                    optimizer_type,
                    self.weightsdense[layer], dw,
                    self.biasdense[layer], db,
                    learning_rate
                )
        for layer in range(self.layercountlstm):
          dwf_sum = np.zeros_like(self.forgetweight[layer])
          dwi_sum = np.zeros_like(self.inputweight[layer])
          dwc_sum = np.zeros_like(self.cellweight[layer])
          dwo_sum = np.zeros_like(self.outputweight[layer])
          for timestep in range(x.shape[1]-1,-1,-1):
             if timestep==x.shape[1]-1:
               dhidden=delta
               dcell=dhidden*self.outputgate[timestep,layer]*(1-np.tanh(self.ct[timestep,layer])**2)
             else:
               dcell=dcell*self.forgetgate[timestep+1,layer]
               fo=np.tanh(self.ct[timestep+1,layer])*self.activationderivative(self.ot[timestep+1,layer].'sigmoid')
               fo=np.dot(fg,self.hidden_outputweight[layer].T)
               ff=self.ot[timestep+1,layer]*self.activationderivative(self.ct[timestep+1,layer],'tanh')*self.ct[timestep,layer]*self.activationderivative(self.ft[timestep+1],'sigmoid')
               ff=np.dot(ff,hidden_forgetweight[layer].T)
               fi=self.ot[timestep+1,layer]*self.activationderivative(self.ct[timestep+1,layer],'tanh')*self.activationderivative(self.it[timestep+1],'sigmoid')*self.ct_candidate[timestep+1]
               fi=np.dot(fi,hidden_inputweight[layer].T)
               fc=self.ot[timestep+1,layer]*self.activationderivative(self.ct[timestep+1,layer],'tanh')*self.activationderivative(self.ct_candidate[timestep+1],'tanh')*self.it[timestep+1]
               fc=np.dot(fc,hidden_cellweight[layer].T)

               dhidden=fo+ff+fi+fc
               dcell = dcell * self.ft[timestep+1,layer] + dhidden * self.ot[timestep,layer] * (1 - np.tanh(self.ct[timestep,layer])**2)
            dwf,dwo,dwc_candidate,dwo=self.backpropagationLSTM(delta,dcell,timestep,layer,learning_rate,x,optimizer_type)
            dwf_sum += dwf
            dwi_sum += dwi
            dwc_sum += dwc
            dwo_sum += dwo
          self.forgetweight[layer], self.forgetbias[layer] = self.optimizers['forget'][layer].optimizer(
                    optimizer_type,
                    self.forgetweight[layer], dwf_sum,
                    self.forgetbias[layer], np.sum(dwf_sum, axis=0, keepdims=True),
                    learning_rate
                )
          self.inputweight[layer], self.inputbias[layer] = self.optimizers['input'][layer].optimizer(
                    optimizer_type,
                    self.inputweight[layer], dwi_sum,
                    self.inputbias[layer], np.sum(dwi_sum, axis=0, keepdims=True),
                    learning_rate
                )
          self.cellweight[layer], self.cellbias[layer] = self.optimizers['cell'][layer].optimizer(
                    optimizer_type,
                    self.cellweight[layer], dwc_sum,
                    self.cellbias[layer], np.sum(dwc_sum, axis=0, keepdims=True),
                    learning_rate
                )
         self.outputweight[layer], self.outputbias[layer] = self.optimizers['output'][layer].optimizer(
                    optimizer_type,
                    self.outputweight[layer], dwo_sum,
                    self.outputbias[layer], np.sum(dwo_sum, axis=0, keepdims=True),
                    learning_rate
                )


    def predict(self,x):
      for layer in range(self.layercountlstm):
          self.ht[0,layer]=np.zeros((batch_size,self.neurons[layer]))
          self.ct[0,layer]=np.zeros((batch_size,self.neurons[layer]))
        for timestep in range(1,x.shape[1]):
          for layer in range(self.layercountlstm):
            if layer==0:
              self.forwardpropagationLSTM(x[:,timestep,:],timestep,layer)
            else:
              self.forwardpropagationLSTM(self.ht[timestep,layer-1],timestep,layer)
        denseinput=self.ht[x.shape[-1],self.layercountlstm-1]
        for layer in range(self.layercountdense):
            denseinput=self.forwardpropagationdense(denseinput,layer)
     return denseinput


class Optimizer:

    def __init__(self):
        self.m_w = 0
        self.v_w = 0
        self.m_b = 0
        self.v_b = 0
        self.t = 0

    def optimizer(self, optimizer, w, dw, b, db, learning_rate, beta1=0.9, beta2=0.99, eps=1e-8):
        self.t += 1

        if optimizer == 'adam':
            self.m_w=beta1*self.m_w +(1-beta1)*dw
            self.m_b=beta1*self.m_b +(1-beta1)*db
            self.v_w=beta2*self.v_w + (1-beta2)*dw ** 2
            self.v_b=beta2*self.v_b + (1-beta2)*db ** 2

            m_w_hat=self.m_w / (1-beta1**self.t)
            m_b_hat=self.m_b / (1-beta1**self.t)
            v_w_hat=self.v_w / (1-beta2**self.t)
            v_b_hat=self.v_b / (1-beta2**self.t)

            w-=learning_rate*m_w_hat/(np.sqrt(v_w_hat)+eps)
            b-=learning_rate*m_b_hat/(np.sqrt(v_b_hat)+eps)

        elif optimizer=='rmsprop':
            self.v_w=beta1*self.v_w + (1 - beta1)*dw ** 2
            self.v_b=beta1*self.v_b + (1 - beta1) * db ** 2
            w-=learning_rate*dw / (np.sqrt(self.v_w) + eps)
            b-=learning_rate*db / (np.sqrt(self.v_b) + eps)

        elif optimizer == 'sgd':
            w -= learning_rate*dw
            b -= learning_rate*db

        elif optimizer == 'momentum':
            self.v_w =beta1*self.v_w+learning_rate*dw
            self.v_b =beta1*self.v_b+learning_rate*db
            w -= self.v_w
            b -= self.v_b

        else:
            raise ValueError("Unsupported optimizer function.")

        return w, b








